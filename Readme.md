# Schedule and Upcoming Todos

## Completed:

- **8-April-2024:** Section 1 to 3.
- **9-April-2024:** Section 4 to 5.

## Later Todos:

1. Understand the architecture and code for x-vector, ECAPA, wav2vec 2.0, wavLM, and Unispeech-SAT.

2. Try Hugging Face spaces and models:
   - [Speech Emotion Recognition](https://huggingface.co/spaces/ZionC27/Speech-Emotion-Recognition)

3. Learn more about the SUPERB benchmark.

4. Understand wav2vec 2.0 with Hugging Face transformers:
   [Wav2Vec2.0 Guide](https://mohitmayank.com/a_lazy_data_science_guide/audio_intelligence/wav2vec2/)

5. **10 April:** Section 1 to 3.
6. **11 April:** Section 4 to 6.
7. **12 April:** Appendix 9.1 to 9.3.

8. Understand the code for paper1, i.e., the pipeline for the CREMA-D evaluation dataset, and SER.

## Planned Ahead (23 April):

1. Go through dimensionality reduction methods:
   - SVD, PCA, and LDA.
   - GRP, KPCA, and Autoencoders (Single-hidden layer).

2. Create an environment for the pipeline.

3. Go through the pipeline and run it end-to-end on GPU.

![OpenAI Logo](images/output_report.png "OpenAI Logo")

4. Apply dimensionality reduction methods learned above in the SER pipeline:
   - PCA, KPCA, SVD, LDA, GRP.

   **Experiments:**
   - Embedding size of 120.
   - Embedding size of 240.

   *All results are updated in* `results_document.md` *document.*

5. **MLP Experiments:**
   - 240 Embedding size.
   - 120 Embedding size.

   **SVM Experiment:**

6. Experiment with random selection from embedding vector with sizes 384, 240, and 120.

7. Modify the pipeline for SER and use multiple PTMs from Hugging Face.

8. Code walkthrough for paper 2.
