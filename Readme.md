# This File will contain schedule or upcoming todos:

8-April-2024: Section 1 to 3.(Done):
9-April-2024: Section 4 to 5.(Done)

Later Todos:
1) Understand the architecture and code for x-vector, ECAPA, wav2vec 2.0, wavLM, and Unispeech-SAT.(Done)

2) Try hugging face spaces and models(Done)
    https://huggingface.co/spaces/ZionC27/Speech-Emotion-Recognition

3) Learn more about the SUPERB benchmark.

4) wav2vec 2.0 with huggingface transformers:
https://mohitmayank.com/a_lazy_data_science_guide/audio_intelligence/wav2vec2/

Section 1 to 3 (10 april):(Done)

Section 4 to 6 (11 april):(Done)

Appendix 9.1 to 9,3 (12 april) (Done)

Understand the code for paper1, i.e the pipeline for CREMA-D evaluation dataset, and SER.(Done)

Plan ahead(23 April):

1) Go through dimensionality reduction methods SVD, PCA, and LDA(Done)

 GRP, KPCA and Autoencoders(Single-hiddle layer)(Done)

2) Create environment for the pipeline.(Done)

3) We will go through pipeline and run it end-to-end on gpu. (Done)

![OpenAI Logo](images/output_report.png "OpenAI Logo")

4) Apply dimensionality reduction methods learned above in the SER pipeline.(TODO):

4a) PCA, KPCA, SVD, LDA, GRP:

Experiment 1(todo): Embedding size of 120.(Compare the result) with same model.

Experiment 2(todo): Embedding size of 240.(Compare the result) with same model.


5) Modify the pipeline for SER and use multiple PTM's((TODO)) from huggingface. (TODO)



6) Code walkthrough for paper2.(TODO)

